# -*- coding: utf-8 -*-
"""GPT2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16aBkzcQTorbeJzF6Fm1rr4cK_qT9mnpt
"""

pip install transformers torch datasets

import pandas as pd
from datasets import Dataset

# Load your CSV file
df = pd.read_csv('offerings - Reviews_Preprocessed.csv')

# Convert to HuggingFace Dataset format
dataset = Dataset.from_pandas(df)

# Split into train and validation sets
dataset = dataset.train_test_split(test_size=0.2, seed=42)

from transformers import GPT2Tokenizer
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokenizer.pad_token = tokenizer.eos_token  # Set padding token

def tokenize_function(examples):
    return tokenizer(examples["preprocessed_reviews"], truncation=True, max_length=512)
    tokenized_datasets = dataset.map(tokenize_function, batched=True)

from transformers import GPT2LMHeadModel

model = GPT2LMHeadModel.from_pretrained('gpt2')

import os
os.environ["WANDB_DISABLED"] = "true"  # Disable wandb if you don't want it

from transformers import GPT2Tokenizer, GPT2LMHeadModel, TrainingArguments, Trainer, DataCollatorForLanguageModeling
from datasets import Dataset
import pandas as pd

# Load data
df = pd.read_csv('offerings - Reviews_Preprocessed.csv')
dataset = Dataset.from_pandas(df)
dataset = dataset.train_test_split(test_size=0.2, seed=42)

# Tokenize
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokenizer.pad_token = tokenizer.eos_token

def tokenize_function(examples):
    return tokenizer(examples["preprocessed_reviews"], truncation=True, max_length=512)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Model and training setup
model = GPT2LMHeadModel.from_pretrained('gpt2')
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

training_args = TrainingArguments(
    output_dir="./gpt2-hotel-reviews",
    run_name="gpt2-hotel-reviews-exp1",  # Descriptive run name
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    save_steps=10_000,
    save_total_limit=2,
    logging_steps=500,
    learning_rate=5e-5,
    warmup_steps=500,
    weight_decay=0.01,
    fp16=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    data_collator=data_collator,
)

trainer.train()

model.save_pretrained("./gpt2-hotel-reviews-final")
tokenizer.save_pretrained("./gpt2-hotel-reviews-final")

from transformers import pipeline

generator = pipeline('text-generation', model='./gpt2-hotel-reviews-final', tokenizer='./gpt2-hotel-reviews-final')

prompt = "The hotel was clean and"
generated_text = generator(prompt, max_length=100, num_return_sequences=1)
print(generated_text[0]['generated_text'])

import gradio as gr
from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline
import torch

# Load your trained model and tokenizer
model_path = "./gpt2-hotel-reviews-final"  # Path to your saved model
tokenizer = GPT2Tokenizer.from_pretrained(model_path)
model = GPT2LMHeadModel.from_pretrained(model_path)

# Create text generation pipeline
generator = pipeline(
    'text-generation',
    model=model,
    tokenizer=tokenizer,
    device=0 if torch.cuda.is_available() else -1
)

def generate_review(prompt, max_length, temperature, num_samples):
    try:
        # Generate text
        outputs = generator(
            prompt,
            max_length=max_length,
            temperature=temperature,
            num_return_sequences=num_samples,
            do_sample=True,
            top_k=50,
            top_p=0.95,
            no_repeat_ngram_size=2
        )

        # Extract generated texts
        generated_reviews = [output['generated_text'] for output in outputs]

        # Format output
        if num_samples == 1:
            return generated_reviews[0]
        else:
            return "\n\n---\n\n".join([f"Option {i+1}:\n{review}"
                                     for i, review in enumerate(generated_reviews)])

    except Exception as e:
        return f"Error generating text: {str(e)}"

# Create Gradio interface
with gr.Blocks(title="Hotel Review Generator") as demo:
    gr.Markdown("# üè® AI Hotel Review Generator")
    gr.Markdown("Generate realistic hotel reviews using GPT-2 fine-tuned on hotel review data")

    with gr.Row():
        with gr.Column():
            prompt = gr.Textbox(
                label="Start your review (or leave empty for random generation)",
                placeholder="The hotel was clean and...",
                lines=3
            )

            with gr.Accordion("Advanced Settings", open=False):
                max_length = gr.Slider(
                    minimum=50,
                    maximum=500,
                    value=150,
                    label="Max Length"
                )
                temperature = gr.Slider(
                    minimum=0.1,
                    maximum=1.5,
                    value=0.7,
                    label="Creativity (Temperature)"
                )
                num_samples = gr.Slider(
                    minimum=1,
                    maximum=5,
                    step=1,
                    value=1,
                    label="Number of samples to generate"
                )

            generate_btn = gr.Button("Generate Review", variant="primary")

        with gr.Column():
            output = gr.Textbox(
                label="Generated Review",
                lines=10,
                interactive=False
            )

    # Example prompts
    examples = gr.Examples(
        examples=[
            ["The hotel staff was"],
            ["The room was clean but"],
            ["I loved the breakfast"],
            ["The location was perfect for"],
            ["Unfortunately, the"]
        ],
        inputs=prompt,
        label="Try these example prompts"
    )

    generate_btn.click(
        fn=generate_review,
        inputs=[prompt, max_length, temperature, num_samples],
        outputs=output
    )

# Launch the interface
demo.launch(share=True)  # Set share=False if you don't want a public link