# -*- coding: utf-8 -*-
"""Crew AI MCP

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11du_6p2qAoWzbEzMdPBsTcT8haVeEzyW
"""

!pip install langchain chromadb gradio sentence-transformers transformers langchain-community
from langchain_community.embeddings import HuggingFaceEmbeddings

# from langchain.embeddings import HuggingFaceEmbeddings # old import
from langchain_community.embeddings import HuggingFaceEmbeddings # new import
from langchain.vectorstores import Chroma
from langchain.text_splitter import CharacterTextSplitter
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from langchain.llms import HuggingFacePipeline
import gradio as gr

model_name = "google/flan-t5-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
hf_pipeline = pipeline("text2text-generation", model=model, tokenizer=tokenizer)
llm = HuggingFacePipeline(pipeline=hf_pipeline)

embedding = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

embedding = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
docs = [
    "Model Context Protocol (MCP) helps manage memory and state across LLM systems.",
    "Retrieval-Augmented Generation (RAG) uses external documents with LLMs for better answers.",
    "CrewAI allows agent-based design using roles like researcher, planner, and writer."
]

text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=10)
split_docs = text_splitter.create_documents(docs)

vectordb = Chroma.from_documents(split_docs, embedding)
retriever = vectordb.as_retriever()

class MCPContext:
    def __init__(self):
        self.data = {"query": "", "retrieved_docs": [], "plan": "", "final_output": ""}
    def update(self, key, value): self.data[key] = value
    def get(self, key): return self.data.get(key, "")
    def all(self): return self.data

# CrewAI-style agents
class ResearchAgent:
    def __init__(self, retriever): self.retriever = retriever
    def run(self, context):
        query = context.get("query")
        docs = self.retriever.get_relevant_documents(query)
        context.update("retrieved_docs", [doc.page_content for doc in docs])
        return "Documents retrieved."

class PlannerAgent:
    def run(self, context):
        query = context.get("query")
        docs = context.get("retrieved_docs")
        plan = f"To answer '{query}', use the following facts:\n" + "\n".join(docs[:2])
        context.update("plan", plan)
        return "Plan created."

class WriterAgent:
    def __init__(self, llm): self.llm = llm
    def run(self, context):
        prompt = f"Based on this plan, write a response:\n{context.get('plan')}"
        # The issue is here: the LLM might return an empty string or an unexpected format
        # Let's add a check and a more robust way to get the generated text
        try:
            response = self.llm(prompt)
            # Check the type and structure of the response
            if isinstance(response, list) and len(response) > 0:
                if isinstance(response[0], dict) and 'generated_text' in response[0]:
                    answer = response[0]['generated_text']
                else:
                    # If it's a list but not a list of dicts with 'generated_text'
                    answer = f"LLM response is a list, but first element is not a dict with 'generated_text'. Response structure: {response}"
            elif isinstance(response, str): # Handle cases where it might return just a string
                 answer = response
            else:
                # If the response is neither a list nor a string
                answer = f"Could not generate a response based on the plan. Unexpected LLM response format: {type(response)._name_} - {response}"
        except Exception as e:
            answer = f"An error occurred during text generation: {e}"

        context.update("final_output", answer)
        return "Answer written."

# MCP Orchestrator
class MCPSystem:
    def __init__(self):
        self.context = MCPContext()
        self.agents = [ResearchAgent(retriever), PlannerAgent(), WriterAgent(llm)]
    def run(self, user_query):
        self.context.update("query", user_query)
        for agent in self.agents: agent.run(self.context)
        return self.context.get("final_output")

system = MCPSystem()
gr.Interface(
    fn=system.run,
    inputs="text",
    outputs="text",
    title="ðŸ§  CrewAI-style Model Context Protocol (MCP)",
    description="Agents work together to answer using shared context (MCP) and free LLMs."
).launch()